{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d2ad766",
   "metadata": {},
   "source": [
    "# Extracting Title & Article from the URL and Computing Variables by Analyzing the Data.\n",
    "\n",
    "## Step - 1 :- Importing requried Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9e7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import pyphen  \n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b09d7f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>50921.0</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>51382.8</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>51844.6</td>\n",
       "      <td>https://insights.blackcoffer.com/what-are-the-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>52306.4</td>\n",
       "      <td>https://insights.blackcoffer.com/marketing-dri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>52768.2</td>\n",
       "      <td>https://insights.blackcoffer.com/continued-dem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      URL_ID                                                URL\n",
       "0      123.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "1      321.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "2     2345.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "3     4321.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "4      432.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "..       ...                                                ...\n",
       "109  50921.0  https://insights.blackcoffer.com/coronavirus-i...\n",
       "110  51382.8  https://insights.blackcoffer.com/coronavirus-i...\n",
       "111  51844.6  https://insights.blackcoffer.com/what-are-the-...\n",
       "112  52306.4  https://insights.blackcoffer.com/marketing-dri...\n",
       "113  52768.2  https://insights.blackcoffer.com/continued-dem...\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('input.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939aef8a",
   "metadata": {},
   "source": [
    "## Creating URL Links List to pass one by one through URL Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dfe33bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-2/', 'https://insights.blackcoffer.com/rise-of-e-health-and-its-impact-on-humans-by-the-year-2030/', 'https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030-2/', 'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-2/', 'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-2-2/', 'https://insights.blackcoffer.com/rise-of-chatbots-and-its-impact-on-customer-support-by-the-year-2040/', 'https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030/', 'https://insights.blackcoffer.com/how-does-marketing-influence-businesses-and-consumers/', 'https://insights.blackcoffer.com/how-advertisement-increase-your-market-value/', 'https://insights.blackcoffer.com/negative-effects-of-marketing-on-society/', 'https://insights.blackcoffer.com/how-advertisement-marketing-affects-business/', 'https://insights.blackcoffer.com/rising-it-cities-will-impact-the-economy-environment-infrastructure-and-city-life-by-the-year-2035/', 'https://insights.blackcoffer.com/rise-of-ott-platform-and-its-impact-on-entertainment-industry-by-the-year-2030/', 'https://insights.blackcoffer.com/rise-of-electric-vehicles-and-its-impact-on-livelihood-by-2040/', 'https://insights.blackcoffer.com/rise-of-electric-vehicle-and-its-impact-on-livelihood-by-the-year-2040/', 'https://insights.blackcoffer.com/oil-prices-by-the-year-2040-and-how-it-will-impact-the-world-economy/', 'https://insights.blackcoffer.com/an-outlook-of-healthcare-by-the-year-2040-and-how-it-will-impact-human-lives/', 'https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/', 'https://insights.blackcoffer.com/what-if-the-creation-is-taking-over-the-creator/', 'https://insights.blackcoffer.com/what-jobs-will-robots-take-from-humans-in-the-future/', 'https://insights.blackcoffer.com/will-machine-replace-the-human-in-the-future-of-work/', 'https://insights.blackcoffer.com/will-ai-replace-us-or-work-with-us/', 'https://insights.blackcoffer.com/man-and-machines-together-machines-are-more-diligent-than-humans-blackcoffe/', 'https://insights.blackcoffer.com/in-future-or-in-upcoming-years-humans-and-machines-are-going-to-work-together-in-every-field-of-work/', 'https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/', 'https://insights.blackcoffer.com/how-machine-learning-will-affect-your-business/', 'https://insights.blackcoffer.com/deep-learning-impact-on-areas-of-e-learning/', 'https://insights.blackcoffer.com/how-to-protect-future-data-and-its-privacy-blackcoffer/', 'https://insights.blackcoffer.com/how-machines-ai-automations-and-robo-human-are-effective-in-finance-and-banking/', 'https://insights.blackcoffer.com/ai-human-robotics-machine-future-planet-blackcoffer-thinking-jobs-workplace/', 'https://insights.blackcoffer.com/how-ai-will-change-the-world-blackcoffer/', 'https://insights.blackcoffer.com/future-of-work-how-ai-has-entered-the-workplace/', 'https://insights.blackcoffer.com/ai-tool-alexa-google-assistant-finance-banking-tool-future/', 'https://insights.blackcoffer.com/ai-healthcare-revolution-ml-technology-algorithm-google-analytics-industrialrevolution/', 'https://insights.blackcoffer.com/all-you-need-to-know-about-online-marketing/', 'https://insights.blackcoffer.com/evolution-of-advertising-industry/', 'https://insights.blackcoffer.com/how-data-analytics-can-help-your-business-respond-to-the-impact-of-covid-19/', 'https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/', 'https://insights.blackcoffer.com/environmental-impact-of-the-covid-19-pandemic-lesson-for-the-future/', 'https://insights.blackcoffer.com/how-data-analytics-and-ai-are-used-to-halt-the-covid-19-pandemic/', 'https://insights.blackcoffer.com/difference-between-artificial-intelligence-machine-learning-statistics-and-data-mining/', 'https://insights.blackcoffer.com/how-python-became-the-first-choice-for-data-science/', 'https://insights.blackcoffer.com/how-google-fit-measure-heart-and-respiratory-rates-using-a-phone/', 'https://insights.blackcoffer.com/what-is-the-future-of-mobile-apps/', 'https://insights.blackcoffer.com/impact-of-ai-in-health-and-medicine/', 'https://insights.blackcoffer.com/telemedicine-what-patients-like-and-dislike-about-it/', 'https://insights.blackcoffer.com/how-we-forecast-future-technologies/', 'https://insights.blackcoffer.com/can-robots-tackle-late-life-loneliness/', 'https://insights.blackcoffer.com/embedding-care-robots-into-society-socio-technical-considerations/', 'https://insights.blackcoffer.com/management-challenges-for-future-digitalization-of-healthcare-services/', 'https://insights.blackcoffer.com/are-we-any-closer-to-preventing-a-nuclear-holocaust/', 'https://insights.blackcoffer.com/will-technology-eliminate-the-need-for-animal-testing-in-drug-development/', 'https://insights.blackcoffer.com/will-we-ever-understand-the-nature-of-consciousness/', 'https://insights.blackcoffer.com/will-we-ever-colonize-outer-space/', 'https://insights.blackcoffer.com/what-is-the-chance-homo-sapiens-will-survive-for-the-next-500-years/', 'https://insights.blackcoffer.com/why-does-your-business-need-a-chatbot/', 'https://insights.blackcoffer.com/how-you-lead-a-project-or-a-team-without-any-technical-expertise/', 'https://insights.blackcoffer.com/can-you-be-great-leader-without-technical-expertise/', 'https://insights.blackcoffer.com/how-does-artificial-intelligence-affect-the-environment/', 'https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes-2/', 'https://insights.blackcoffer.com/is-perfection-the-greatest-enemy-of-productivity/', 'https://insights.blackcoffer.com/global-financial-crisis-2008-causes-effects-and-its-solution/', 'https://insights.blackcoffer.com/gender-diversity-and-equality-in-the-tech-industry/', 'https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes/', 'https://insights.blackcoffer.com/how-small-business-can-survive-the-coronavirus-crisis/', 'https://insights.blackcoffer.com/impacts-of-covid-19-on-vegetable-vendors-and-food-stalls/', 'https://insights.blackcoffer.com/impacts-of-covid-19-on-vegetable-vendors/', 'https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-tourism-aviation-industries/', 'https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-sports-events-around-the-world/', 'https://insights.blackcoffer.com/changing-landscape-and-emerging-trends-in-the-indian-it-ites-industry/', 'https://insights.blackcoffer.com/online-gaming-adolescent-online-gaming-effects-demotivated-depression-musculoskeletal-and-psychosomatic-symptoms/', 'https://insights.blackcoffer.com/human-rights-outlook/', 'https://insights.blackcoffer.com/how-voice-search-makes-your-business-a-successful-business/', 'https://insights.blackcoffer.com/how-the-covid-19-crisis-is-redefining-jobs-and-services/', 'https://insights.blackcoffer.com/how-to-increase-social-media-engagement-for-marketers/', 'https://insights.blackcoffer.com/impacts-of-covid-19-on-streets-sides-food-stalls/', 'https://insights.blackcoffer.com/coronavirus-impact-on-energy-markets-2/', 'https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-5/', 'https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-4/', 'https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-2/', 'https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-3/', 'https://insights.blackcoffer.com/travel-and-tourism-outlook/', 'https://insights.blackcoffer.com/gaming-disorder-and-effects-of-gaming-on-health/', 'https://insights.blackcoffer.com/what-is-the-repercussion-of-the-environment-due-to-the-covid-19-pandemic-situation/', 'https://insights.blackcoffer.com/what-is-the-repercussion-of-the-environment-due-to-the-covid-19-pandemic-situation-2/', 'https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-office-space-and-co-working-industries/', 'https://insights.blackcoffer.com/contribution-of-handicrafts-visual-arts-literature-in-the-indian-economy/', 'https://insights.blackcoffer.com/how-covid-19-is-impacting-payment-preferences/', 'https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-2/', 'https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis/', 'https://insights.blackcoffer.com/covid-19-how-have-countries-been-responding/', 'https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-2/', 'https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-3/', 'https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-3/', 'https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work/', 'https://insights.blackcoffer.com/covid-19-how-have-countries-been-responding-2/', 'https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-4/', 'https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-2/', 'https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-3/', 'https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-4/', 'https://insights.blackcoffer.com/why-scams-like-nirav-modi-happen-with-indian-banks/', 'https://insights.blackcoffer.com/impact-of-covid-19-on-the-global-economy/', 'https://insights.blackcoffer.com/impact-of-covid-19coronavirus-on-the-indian-economy-2/', 'https://insights.blackcoffer.com/impact-of-covid-19-on-the-global-economy-2/', 'https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-indian-economy-3/', 'https://insights.blackcoffer.com/should-celebrities-be-allowed-to-join-politics/', 'https://insights.blackcoffer.com/how-prepared-is-india-to-tackle-a-possible-covid-19-outbreak/', 'https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work/', 'https://insights.blackcoffer.com/controversy-as-a-marketing-strategy/', 'https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry/', 'https://insights.blackcoffer.com/coronavirus-impact-on-energy-markets/', 'https://insights.blackcoffer.com/what-are-the-key-policies-that-will-mitigate-the-impacts-of-covid-19-on-the-world-of-work/', 'https://insights.blackcoffer.com/marketing-drives-results-with-a-focus-on-problems/', 'https://insights.blackcoffer.com/continued-demand-for-sustainability/']\n"
     ]
    }
   ],
   "source": [
    "Links = df['URL']\n",
    "Links_List = list(Links)\n",
    "print(Links_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce9f62",
   "metadata": {},
   "source": [
    "## Creating URL_ID List to through Name Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6115ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123.txt', '321.txt', '2345.txt', '4321.txt', '432.txt', '2893.8.txt', '3355.6.txt', '3817.4.txt', '4279.2.txt', '4741.txt', '5202.8.txt', '5664.6.txt', '6126.4.txt', '6588.2.txt', '7050.txt', '7511.8.txt', '7973.6.txt', '8435.4.txt', '8897.2.txt', '9359.txt', '9820.8.txt', '10282.6.txt', '10744.4.txt', '11206.2.txt', '11668.txt', '12129.8.txt', '12591.6.txt', '13053.4.txt', '13515.2.txt', '13977.txt', '14438.8.txt', '14900.6.txt', '15362.4.txt', '15824.2.txt', '16286.txt', '16747.8.txt', '17209.6.txt', '17671.4.txt', '18133.2.txt', '18595.txt', '19056.8.txt', '19518.6.txt', '19980.4.txt', '20442.2.txt', '20904.txt', '21365.8.txt', '21827.6.txt', '22289.4.txt', '22751.2.txt', '23213.txt', '23674.8.txt', '24136.6.txt', '24598.4.txt', '25060.2.txt', '25522.txt', '25983.8.txt', '26445.6.txt', '26907.4.txt', '27369.2.txt', '27831.txt', '28292.8.txt', '28754.6.txt', '29216.4.txt', '29678.2.txt', '30140.txt', '30601.8.txt', '31063.6.txt', '31525.4.txt', '31987.2.txt', '32449.txt', '32910.8.txt', '33372.6.txt', '33834.4.txt', '34296.2.txt', '34758.txt', '35219.8.txt', '35681.6.txt', '36143.4.txt', '36605.2.txt', '37067.txt', '37528.8.txt', '37990.6.txt', '38452.4.txt', '38914.2.txt', '39376.txt', '39837.8.txt', '40299.6.txt', '40761.4.txt', '41223.2.txt', '41685.txt', '42146.8.txt', '42608.6.txt', '43070.4.txt', '43532.2.txt', '43994.txt', '44455.8.txt', '44917.6.txt', '45379.4.txt', '45841.2.txt', '46303.txt', '46764.8.txt', '47226.6.txt', '47688.4.txt', '48150.2.txt', '48612.txt', '49073.8.txt', '49535.6.txt', '49997.4.txt', '50459.2.txt', '50921.txt', '51382.8.txt', '51844.6.txt', '52306.4.txt', '52768.2.txt']\n"
     ]
    }
   ],
   "source": [
    "Name = df['URL_ID']\n",
    "Name_List = list(Name)  \n",
    "File_Names = []\n",
    "for num in Name_List:\n",
    "    if num % 1 == 0:\n",
    "        File_Names.append(str(int(num)) + \".txt\")\n",
    "    else:\n",
    "        \n",
    "        File_Names.append(str(num) + \".txt\")\n",
    "print(File_Names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d0e579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "print(len(File_Names))\n",
    "print(len(Links_List))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d2fd9",
   "metadata": {},
   "source": [
    "## Using For Loop for passing both Names & Links and Extracting Titles & Articles from the URL's by using Python Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd298dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text File saved in Directory as 123.txt\n",
      "Text File saved in Directory as 321.txt\n",
      "Text File saved in Directory as 2345.txt\n",
      "Text File saved in Directory as 4321.txt\n",
      "Text File saved in Directory as 432.txt\n",
      "Text File saved in Directory as 2893.8.txt\n",
      "Text File saved in Directory as 3355.6.txt\n",
      "Text File saved in Directory as 3817.4.txt\n",
      "Text File saved in Directory as 4279.2.txt\n",
      "Text File saved in Directory as 4741.txt\n",
      "Text File saved in Directory as 5202.8.txt\n",
      "Text File saved in Directory as 5664.6.txt\n",
      "Text File saved in Directory as 6126.4.txt\n",
      "Text File saved in Directory as 6588.2.txt\n",
      "Text File saved in Directory as 7050.txt\n",
      "Text File saved in Directory as 7511.8.txt\n",
      "Text File saved in Directory as 7973.6.txt\n",
      "Text File saved in Directory as 8435.4.txt\n",
      "Text File saved in Directory as 8897.2.txt\n",
      "Text File saved in Directory as 9359.txt\n",
      "Text File saved in Directory as 9820.8.txt\n",
      "Text File saved in Directory as 10282.6.txt\n",
      "Text File saved in Directory as 10744.4.txt\n",
      "Text File saved in Directory as 11206.2.txt\n",
      "Text File saved in Directory as 12129.8.txt\n",
      "Text File saved in Directory as 12591.6.txt\n",
      "Text File saved in Directory as 13053.4.txt\n",
      "Text File saved in Directory as 13515.2.txt\n",
      "Text File saved in Directory as 13977.txt\n",
      "Text File saved in Directory as 14438.8.txt\n",
      "Text File saved in Directory as 14900.6.txt\n",
      "Text File saved in Directory as 15362.4.txt\n",
      "Text File saved in Directory as 15824.2.txt\n",
      "Text File saved in Directory as 16286.txt\n",
      "Text File saved in Directory as 16747.8.txt\n",
      "Text File saved in Directory as 17209.6.txt\n",
      "Text File saved in Directory as 18133.2.txt\n",
      "Text File saved in Directory as 18595.txt\n",
      "Text File saved in Directory as 19056.8.txt\n",
      "Text File saved in Directory as 19518.6.txt\n",
      "Text File saved in Directory as 19980.4.txt\n",
      "Text File saved in Directory as 20442.2.txt\n",
      "Text File saved in Directory as 20904.txt\n",
      "Text File saved in Directory as 21365.8.txt\n",
      "Text File saved in Directory as 21827.6.txt\n",
      "Text File saved in Directory as 22289.4.txt\n",
      "Text File saved in Directory as 22751.2.txt\n",
      "Text File saved in Directory as 23213.txt\n",
      "Text File saved in Directory as 23674.8.txt\n",
      "Text File saved in Directory as 24136.6.txt\n",
      "Text File saved in Directory as 24598.4.txt\n",
      "Text File saved in Directory as 25060.2.txt\n",
      "Text File saved in Directory as 25522.txt\n",
      "Text File saved in Directory as 25983.8.txt\n",
      "Text File saved in Directory as 26445.6.txt\n",
      "Text File saved in Directory as 26907.4.txt\n",
      "Text File saved in Directory as 27369.2.txt\n",
      "Text File saved in Directory as 27831.txt\n",
      "Text File saved in Directory as 28292.8.txt\n",
      "Text File saved in Directory as 28754.6.txt\n",
      "Text File saved in Directory as 29216.4.txt\n",
      "Text File saved in Directory as 29678.2.txt\n",
      "Text File saved in Directory as 30140.txt\n",
      "Text File saved in Directory as 30601.8.txt\n",
      "Text File saved in Directory as 31063.6.txt\n",
      "Text File saved in Directory as 31525.4.txt\n",
      "Text File saved in Directory as 31987.2.txt\n",
      "Text File saved in Directory as 32449.txt\n",
      "Text File saved in Directory as 32910.8.txt\n",
      "Text File saved in Directory as 33372.6.txt\n",
      "Text File saved in Directory as 33834.4.txt\n",
      "Text File saved in Directory as 34296.2.txt\n",
      "Text File saved in Directory as 34758.txt\n",
      "Text File saved in Directory as 35219.8.txt\n",
      "Text File saved in Directory as 35681.6.txt\n",
      "Text File saved in Directory as 36143.4.txt\n",
      "Text File saved in Directory as 36605.2.txt\n",
      "Text File saved in Directory as 37067.txt\n",
      "Text File saved in Directory as 37528.8.txt\n",
      "Text File saved in Directory as 37990.6.txt\n",
      "Text File saved in Directory as 38452.4.txt\n",
      "Text File saved in Directory as 38914.2.txt\n",
      "Text File saved in Directory as 39376.txt\n",
      "Text File saved in Directory as 39837.8.txt\n",
      "Text File saved in Directory as 40299.6.txt\n",
      "Text File saved in Directory as 40761.4.txt\n",
      "Text File saved in Directory as 41223.2.txt\n",
      "Text File saved in Directory as 41685.txt\n",
      "Text File saved in Directory as 42146.8.txt\n",
      "Text File saved in Directory as 42608.6.txt\n",
      "Text File saved in Directory as 43070.4.txt\n",
      "Text File saved in Directory as 43532.2.txt\n",
      "Text File saved in Directory as 43994.txt\n",
      "Text File saved in Directory as 44455.8.txt\n",
      "Text File saved in Directory as 44917.6.txt\n",
      "Text File saved in Directory as 45379.4.txt\n",
      "Text File saved in Directory as 45841.2.txt\n",
      "Text File saved in Directory as 46303.txt\n",
      "Text File saved in Directory as 46764.8.txt\n",
      "Text File saved in Directory as 47226.6.txt\n",
      "Text File saved in Directory as 47688.4.txt\n",
      "Text File saved in Directory as 48150.2.txt\n",
      "Text File saved in Directory as 48612.txt\n",
      "Text File saved in Directory as 49073.8.txt\n",
      "Text File saved in Directory as 49535.6.txt\n",
      "Text File saved in Directory as 49997.4.txt\n",
      "Text File saved in Directory as 50459.2.txt\n",
      "Text File saved in Directory as 50921.txt\n",
      "Text File saved in Directory as 51382.8.txt\n",
      "Text File saved in Directory as 51844.6.txt\n",
      "Text File saved in Directory as 52306.4.txt\n",
      "Text File saved in Directory as 52768.2.txt\n"
     ]
    }
   ],
   "source": [
    "for name , link in zip(File_Names , Links_List ):\n",
    "    url = link\n",
    "    req = requests.get(url)\n",
    "    html_page = req.content\n",
    "\n",
    "    soup = BeautifulSoup(html_page,'html.parser')\n",
    "\n",
    "    Title = soup.find('title')\n",
    "    Clean_Title = Title.get_text()\n",
    "\n",
    "    Article = soup.find('div',attrs = {'class' : \"td-post-content tagdiv-type\"})\n",
    "    if Article is None:\n",
    "        Article = soup.find('div',attrs = {'class' : \"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\"})\n",
    "        if Article:\n",
    "            \n",
    "            Clean_Article = Article.get_text()\n",
    "\n",
    "            Title_Article = Clean_Title + \"     \" + Clean_Article\n",
    "\n",
    "            Cleaned_Article = Title_Article.replace('\\n',' ')\n",
    "            Encode = str(Cleaned_Article.encode('utf-8'))\n",
    "            file_name = name\n",
    "            with open(file_name, \"w\") as text_file:\n",
    "                text_file.write(Encode)\n",
    "            print(f\"Text File saved in Directory as {file_name}\")\n",
    "        else:\n",
    "            with open(file_name, \"w\",encoding = 'utf-8') as text_file:\n",
    "                pass\n",
    "    else:\n",
    "        \n",
    "        Clean_Article = Article.get_text()\n",
    "\n",
    "        Title_Article = Clean_Title + \"     \" + Clean_Article\n",
    "\n",
    "        Cleaned_Article = Title_Article.replace('\\n',' ')\n",
    "        Encode = str(Cleaned_Article.encode('utf-8'))\n",
    "        file_name = name\n",
    "        with open(file_name, \"w\") as text_file:\n",
    "            text_file.write(Encode)\n",
    "        print(f\"Text File saved in Directory as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984d9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c19e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\DATA SCIENCE CLASSES\\\\Assignment NLTK'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740b92b",
   "metadata": {},
   "source": [
    "## Computing Various Variables as mentioned in the Text Analysis Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88899a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Function to count Syllables.\n",
    "\n",
    "def count_syllables(word):\n",
    "    dic = pyphen.Pyphen(lang='en_US')\n",
    "    return len(dic.inserted(word).split('-'))\n",
    "\n",
    "# Defined a Function to perform text analysis and compute variables as mentioned in Text Analysis File.\n",
    "\n",
    "def analyze_text(text):\n",
    "    \n",
    "# Creating a Function to read stopwords from the text files and converting them into a list.\n",
    "    \n",
    "    def load_stopwords(stopwords_directory):\n",
    "        stopwords = set()\n",
    "        for filename in os.listdir(stopwords_directory):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(stopwords_directory, filename), 'r', encoding='latin-1') as file:\n",
    "                    stopwords.update(file.read().split())\n",
    "        return stopwords\n",
    "\n",
    "    stopwords_directory = 'D:\\DATA SCIENCE CLASSES\\Assignment NLTK\\StopWords'\n",
    "    stopwords = load_stopwords(stopwords_directory)\n",
    "    \n",
    "# Tokenize and preprocessing the text by removing stopwords.\n",
    "    \n",
    "    preprocessed_text = ' '.join(word for word in text.split() if word.lower() not in stopwords) \n",
    "\n",
    "# Create a TextBlob from the preprocessed text for further analysis.\n",
    "\n",
    "    blob = TextBlob(preprocessed_text) \n",
    "\n",
    "# Performing text analysis on Cleaned Text Data.\n",
    "\n",
    "# Computing positive and negative scores using sentiment analysis.\n",
    "    \n",
    "    sentiment = blob.sentiment\n",
    "    positive_score = sentiment.polarity\n",
    "    negative_score = sentiment.subjectivity\n",
    "\n",
    "# Calculating polarity score.\n",
    "\n",
    "    polarity_score = sentiment.polarity\n",
    "\n",
    "# Calculating subjectivity score.\n",
    "\n",
    "    subjectivity_score = sentiment.subjectivity\n",
    "\n",
    "# Tokenizing the text into sentences.\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences == 0 | len(text.split()) == 0:\n",
    "        avg_sentence_length = 0\n",
    "        \n",
    "# Calculating average sentence length.  \n",
    "\n",
    "    else:  \n",
    "        avg_sentence_length = len(text.split()) / num_sentences\n",
    "    \n",
    "# Calculating percentage of complex words.\n",
    "\n",
    "    words = nltk.word_tokenize(text)\n",
    "    complex_word_count = sum(1 for word in words if len(word) > 2) \n",
    "    if complex_word_count == 0 | len(words) == 0:\n",
    "        percentage_complex_words = 0\n",
    "    else:\n",
    "        \n",
    "        percentage_complex_words = (complex_word_count / len(words)) * 100\n",
    "\n",
    "# Calculating the Fog index.\n",
    "\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "# Calculating the average number of words per sentence.\n",
    "\n",
    "    if len(words) == 0 | num_sentences == 0:\n",
    "        avg_words_per_sentence = 0\n",
    "    else:\n",
    "        avg_words_per_sentence = len(words) / num_sentences\n",
    "\n",
    "# Calculating the word count.\n",
    "\n",
    "    word_count = len(words)\n",
    "\n",
    "# Calculate syllables per word using the count_syllables function.\n",
    "\n",
    "    total_syllables = sum(count_syllables(word) for word in words)\n",
    "\n",
    "# Counting personal pronouns.\n",
    "\n",
    "    personal_pronouns = ['I', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves']\n",
    "    personal_pronoun_count = sum(1 for word in words if word.lower() in personal_pronouns)\n",
    "\n",
    "# Calculating average word length.\n",
    "\n",
    "    if sum(len(word) for word in words) == 0 | word_count == 0:\n",
    "        avg_word_length = 0\n",
    "    else:\n",
    "        avg_word_length = sum(len(word) for word in words) / word_count\n",
    "    if total_syllables == 0 | word_count == 0:\n",
    "        syllables_per_word = 0\n",
    "    else:\n",
    "        syllables_per_word = total_syllables / word_count\n",
    "        \n",
    "# returng the computed values.\n",
    "\n",
    "    return {\n",
    "        'POSITIVE SCORE': positive_score,\n",
    "        'NEGATIVE SCORE': negative_score,\n",
    "        'POLARITY SCORE': polarity_score,\n",
    "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "        'AVG SENTENCE LENGTH': avg_sentence_length,\n",
    "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
    "        'FOG INDEX': fog_index,\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,\n",
    "        'COMPLEX WORD COUNT': complex_word_count,\n",
    "        'WORD COUNT': word_count,\n",
    "        'SYLLABLES PER WORD': syllables_per_word,\n",
    "        'PERSONAL PRONOUNS': personal_pronoun_count,\n",
    "        'AVG WORD LENGTH': avg_word_length\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17663f68",
   "metadata": {},
   "source": [
    "## Saving Processed Data as a Data Frame for further Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4be2e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining a function which will process the text for further analysis.\n",
    "def process_text(text):\n",
    "    analysis = analyze_text(text)\n",
    "    return analysis\n",
    "\n",
    "df = pd.DataFrame(columns=['File_Name', 'Text_Data'])\n",
    "\n",
    "directory = 'D:\\\\DATA SCIENCE CLASSES\\\\Assignment NLTK'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text_data = file.read()\n",
    "            processed_data = process_text(text_data)\n",
    "\n",
    "            df = df.append({'File_Name': filename, 'Text_Data': processed_data}, ignore_index=True)\n",
    "\n",
    "df.to_csv('Processed Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dea286",
   "metadata": {},
   "source": [
    "## Loading the Processed Data for further Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20a8b9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Text_Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10282.6.txt</td>\n",
       "      <td>{'POSITIVE SCORE': -0.002972708450969314, 'NEG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10744.4.txt</td>\n",
       "      <td>{'POSITIVE SCORE': 0.05608372456964006, 'NEGAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11206.2.txt</td>\n",
       "      <td>{'POSITIVE SCORE': 0.0, 'NEGATIVE SCORE': 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12129.8.txt</td>\n",
       "      <td>{'POSITIVE SCORE': 0.11575187969924816, 'NEGAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123.txt</td>\n",
       "      <td>{'POSITIVE SCORE': 0.043158780163730655, 'NEGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>7973.6.txt</td>\n",
       "      <td>{'POSITIVE SCORE': 0.05969540121714033, 'NEGAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>8435.4.txt</td>\n",
       "      <td>{'POSITIVE SCORE': 0.09479140517602053, 'NEGAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>8897.2.txt</td>\n",
       "      <td>{'POSITIVE SCORE': 0.04855851534828808, 'NEGAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>9359.txt</td>\n",
       "      <td>{'POSITIVE SCORE': 0.06777067669172934, 'NEGAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>9820.8.txt</td>\n",
       "      <td>{'POSITIVE SCORE': 0.05469869363674674, 'NEGAT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       File_Name                                          Text_Data\n",
       "0    10282.6.txt  {'POSITIVE SCORE': -0.002972708450969314, 'NEG...\n",
       "1    10744.4.txt  {'POSITIVE SCORE': 0.05608372456964006, 'NEGAT...\n",
       "2    11206.2.txt  {'POSITIVE SCORE': 0.0, 'NEGATIVE SCORE': 0.0,...\n",
       "3    12129.8.txt  {'POSITIVE SCORE': 0.11575187969924816, 'NEGAT...\n",
       "4        123.txt  {'POSITIVE SCORE': 0.043158780163730655, 'NEGA...\n",
       "..           ...                                                ...\n",
       "107   7973.6.txt  {'POSITIVE SCORE': 0.05969540121714033, 'NEGAT...\n",
       "108   8435.4.txt  {'POSITIVE SCORE': 0.09479140517602053, 'NEGAT...\n",
       "109   8897.2.txt  {'POSITIVE SCORE': 0.04855851534828808, 'NEGAT...\n",
       "110     9359.txt  {'POSITIVE SCORE': 0.06777067669172934, 'NEGAT...\n",
       "111   9820.8.txt  {'POSITIVE SCORE': 0.05469869363674674, 'NEGAT...\n",
       "\n",
       "[112 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('Processed Data.csv',index_col = False)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa7bb1",
   "metadata": {},
   "source": [
    "## Checking the Data type in 2nd Column by observing the Output.\n",
    "## We can see that the Data in Index - 1 is in between \" \" , So we can Conclude that the Data is in String Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8fd8068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'POSITIVE SCORE': 0.05608372456964006, 'NEGATIVE SCORE': 0.423013078470825, 'POLARITY SCORE': 0.05608372456964006, 'SUBJECTIVITY SCORE': 0.423013078470825, 'AVG SENTENCE LENGTH': 21.096774193548388, 'PERCENTAGE OF COMPLEX WORDS': 72.51023192360164, 'FOG INDEX': 37.44280244686001, 'AVG NUMBER OF WORDS PER SENTENCE': 23.64516129032258, 'COMPLEX WORD COUNT': 1063, 'WORD COUNT': 1466, 'SYLLABLES PER WORD': 1.5600272851296044, 'PERSONAL PRONOUNS': 30, 'AVG WORD LENGTH': 4.886766712141883}\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Text_Data'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bfb600",
   "metadata": {},
   "source": [
    "## Creating DataFrame from the 2nd DataFrame's 2nd Column which contains Dictionary Format Data under the String Format.\n",
    "## Firstly Converting it from string format to Normal Dictionary Format and after that using it for creating Data Frame Columns based on Keys of Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64a060b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SYLLABLES PER WORD</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10282.6.txt</td>\n",
       "      <td>1405</td>\n",
       "      <td>-0.002973</td>\n",
       "      <td>1.615464</td>\n",
       "      <td>1927</td>\n",
       "      <td>-0.002973</td>\n",
       "      <td>39.776626</td>\n",
       "      <td>26.530303</td>\n",
       "      <td>19</td>\n",
       "      <td>72.911261</td>\n",
       "      <td>29.196970</td>\n",
       "      <td>0.517458</td>\n",
       "      <td>5.028542</td>\n",
       "      <td>0.517458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10744.4.txt</td>\n",
       "      <td>1063</td>\n",
       "      <td>0.056084</td>\n",
       "      <td>1.560027</td>\n",
       "      <td>1466</td>\n",
       "      <td>0.056084</td>\n",
       "      <td>37.442802</td>\n",
       "      <td>21.096774</td>\n",
       "      <td>30</td>\n",
       "      <td>72.510232</td>\n",
       "      <td>23.645161</td>\n",
       "      <td>0.423013</td>\n",
       "      <td>4.886767</td>\n",
       "      <td>0.423013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11206.2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12129.8.txt</td>\n",
       "      <td>602</td>\n",
       "      <td>0.115752</td>\n",
       "      <td>1.486989</td>\n",
       "      <td>807</td>\n",
       "      <td>0.115752</td>\n",
       "      <td>37.806477</td>\n",
       "      <td>19.918919</td>\n",
       "      <td>1</td>\n",
       "      <td>74.597274</td>\n",
       "      <td>21.810811</td>\n",
       "      <td>0.556115</td>\n",
       "      <td>4.727385</td>\n",
       "      <td>0.556115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123.txt</td>\n",
       "      <td>1413</td>\n",
       "      <td>0.043159</td>\n",
       "      <td>1.588901</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.043159</td>\n",
       "      <td>38.837586</td>\n",
       "      <td>20.962500</td>\n",
       "      <td>3</td>\n",
       "      <td>76.131466</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>0.365884</td>\n",
       "      <td>5.192349</td>\n",
       "      <td>0.365884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_Name  COMPLEX WORD COUNT  POLARITY SCORE  SYLLABLES PER WORD  \\\n",
       "0  10282.6.txt                1405       -0.002973            1.615464   \n",
       "1  10744.4.txt                1063        0.056084            1.560027   \n",
       "2  11206.2.txt                   0        0.000000            0.000000   \n",
       "3  12129.8.txt                 602        0.115752            1.486989   \n",
       "4      123.txt                1413        0.043159            1.588901   \n",
       "\n",
       "   WORD COUNT  POSITIVE SCORE  FOG INDEX  AVG SENTENCE LENGTH  \\\n",
       "0        1927       -0.002973  39.776626            26.530303   \n",
       "1        1466        0.056084  37.442802            21.096774   \n",
       "2           0        0.000000   0.000000             0.000000   \n",
       "3         807        0.115752  37.806477            19.918919   \n",
       "4        1856        0.043159  38.837586            20.962500   \n",
       "\n",
       "   PERSONAL PRONOUNS  PERCENTAGE OF COMPLEX WORDS  \\\n",
       "0                 19                    72.911261   \n",
       "1                 30                    72.510232   \n",
       "2                  0                     0.000000   \n",
       "3                  1                    74.597274   \n",
       "4                  3                    76.131466   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  NEGATIVE SCORE  AVG WORD LENGTH  \\\n",
       "0                         29.196970        0.517458         5.028542   \n",
       "1                         23.645161        0.423013         4.886767   \n",
       "2                          0.000000        0.000000         0.000000   \n",
       "3                         21.810811        0.556115         4.727385   \n",
       "4                         23.200000        0.365884         5.192349   \n",
       "\n",
       "   SUBJECTIVITY SCORE  \n",
       "0            0.517458  \n",
       "1            0.423013  \n",
       "2            0.000000  \n",
       "3            0.556115  \n",
       "4            0.365884  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the dictionary data into individual columns.\n",
    "\n",
    "# First, we'll convert the string representation of dictionaries into actual dictionaries\n",
    "text_df = df1\n",
    "text_df['Text_Data'] = text_df['Text_Data'].apply(eval)\n",
    "\n",
    "unique_keys = set()\n",
    "for data_dict in text_df['Text_Data']:\n",
    "    unique_keys.update(data_dict.keys())\n",
    "\n",
    "for key in unique_keys:\n",
    "    text_df[key] = text_df['Text_Data'].apply(lambda x: x.get(key, None))\n",
    "\n",
    "text_df = text_df.drop(columns=['Text_Data'])\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f0020",
   "metadata": {},
   "source": [
    "## Found that the elements in DataFrame are in String Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30313b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10744.4.txt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['File_Name'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c42958",
   "metadata": {},
   "source": [
    "## Removing the \".txt\" extension from the \"File_Name\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "934ce4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SYLLABLES PER WORD</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10282.6</td>\n",
       "      <td>1405</td>\n",
       "      <td>-0.002973</td>\n",
       "      <td>1.615464</td>\n",
       "      <td>1927</td>\n",
       "      <td>-0.002973</td>\n",
       "      <td>39.776626</td>\n",
       "      <td>26.530303</td>\n",
       "      <td>19</td>\n",
       "      <td>72.911261</td>\n",
       "      <td>29.196970</td>\n",
       "      <td>0.517458</td>\n",
       "      <td>5.028542</td>\n",
       "      <td>0.517458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10744.4</td>\n",
       "      <td>1063</td>\n",
       "      <td>0.056084</td>\n",
       "      <td>1.560027</td>\n",
       "      <td>1466</td>\n",
       "      <td>0.056084</td>\n",
       "      <td>37.442802</td>\n",
       "      <td>21.096774</td>\n",
       "      <td>30</td>\n",
       "      <td>72.510232</td>\n",
       "      <td>23.645161</td>\n",
       "      <td>0.423013</td>\n",
       "      <td>4.886767</td>\n",
       "      <td>0.423013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11206.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12129.8</td>\n",
       "      <td>602</td>\n",
       "      <td>0.115752</td>\n",
       "      <td>1.486989</td>\n",
       "      <td>807</td>\n",
       "      <td>0.115752</td>\n",
       "      <td>37.806477</td>\n",
       "      <td>19.918919</td>\n",
       "      <td>1</td>\n",
       "      <td>74.597274</td>\n",
       "      <td>21.810811</td>\n",
       "      <td>0.556115</td>\n",
       "      <td>4.727385</td>\n",
       "      <td>0.556115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123.0</td>\n",
       "      <td>1413</td>\n",
       "      <td>0.043159</td>\n",
       "      <td>1.588901</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.043159</td>\n",
       "      <td>38.837586</td>\n",
       "      <td>20.962500</td>\n",
       "      <td>3</td>\n",
       "      <td>76.131466</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>0.365884</td>\n",
       "      <td>5.192349</td>\n",
       "      <td>0.365884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   File_Name  COMPLEX WORD COUNT  POLARITY SCORE  SYLLABLES PER WORD  \\\n",
       "0    10282.6                1405       -0.002973            1.615464   \n",
       "1    10744.4                1063        0.056084            1.560027   \n",
       "2    11206.2                   0        0.000000            0.000000   \n",
       "3    12129.8                 602        0.115752            1.486989   \n",
       "4      123.0                1413        0.043159            1.588901   \n",
       "\n",
       "   WORD COUNT  POSITIVE SCORE  FOG INDEX  AVG SENTENCE LENGTH  \\\n",
       "0        1927       -0.002973  39.776626            26.530303   \n",
       "1        1466        0.056084  37.442802            21.096774   \n",
       "2           0        0.000000   0.000000             0.000000   \n",
       "3         807        0.115752  37.806477            19.918919   \n",
       "4        1856        0.043159  38.837586            20.962500   \n",
       "\n",
       "   PERSONAL PRONOUNS  PERCENTAGE OF COMPLEX WORDS  \\\n",
       "0                 19                    72.911261   \n",
       "1                 30                    72.510232   \n",
       "2                  0                     0.000000   \n",
       "3                  1                    74.597274   \n",
       "4                  3                    76.131466   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  NEGATIVE SCORE  AVG WORD LENGTH  \\\n",
       "0                         29.196970        0.517458         5.028542   \n",
       "1                         23.645161        0.423013         4.886767   \n",
       "2                          0.000000        0.000000         0.000000   \n",
       "3                         21.810811        0.556115         4.727385   \n",
       "4                         23.200000        0.365884         5.192349   \n",
       "\n",
       "   SUBJECTIVITY SCORE  \n",
       "0            0.517458  \n",
       "1            0.423013  \n",
       "2            0.000000  \n",
       "3            0.556115  \n",
       "4            0.365884  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = text_df\n",
    "\n",
    "df['File_Name'] = df['File_Name'].str.replace('.txt', '')\n",
    "\n",
    "df['File_Name'] = df['File_Name'].astype(float)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5bfa8",
   "metadata": {},
   "source": [
    "## Renaming Column Name to merge 2 DataFrames with common column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8580910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SYLLABLES PER WORD</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10282.6</td>\n",
       "      <td>1405</td>\n",
       "      <td>-0.002973</td>\n",
       "      <td>1.615464</td>\n",
       "      <td>1927</td>\n",
       "      <td>-0.002973</td>\n",
       "      <td>39.776626</td>\n",
       "      <td>26.530303</td>\n",
       "      <td>19</td>\n",
       "      <td>72.911261</td>\n",
       "      <td>29.196970</td>\n",
       "      <td>0.517458</td>\n",
       "      <td>5.028542</td>\n",
       "      <td>0.517458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10744.4</td>\n",
       "      <td>1063</td>\n",
       "      <td>0.056084</td>\n",
       "      <td>1.560027</td>\n",
       "      <td>1466</td>\n",
       "      <td>0.056084</td>\n",
       "      <td>37.442802</td>\n",
       "      <td>21.096774</td>\n",
       "      <td>30</td>\n",
       "      <td>72.510232</td>\n",
       "      <td>23.645161</td>\n",
       "      <td>0.423013</td>\n",
       "      <td>4.886767</td>\n",
       "      <td>0.423013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11206.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12129.8</td>\n",
       "      <td>602</td>\n",
       "      <td>0.115752</td>\n",
       "      <td>1.486989</td>\n",
       "      <td>807</td>\n",
       "      <td>0.115752</td>\n",
       "      <td>37.806477</td>\n",
       "      <td>19.918919</td>\n",
       "      <td>1</td>\n",
       "      <td>74.597274</td>\n",
       "      <td>21.810811</td>\n",
       "      <td>0.556115</td>\n",
       "      <td>4.727385</td>\n",
       "      <td>0.556115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123.0</td>\n",
       "      <td>1413</td>\n",
       "      <td>0.043159</td>\n",
       "      <td>1.588901</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.043159</td>\n",
       "      <td>38.837586</td>\n",
       "      <td>20.962500</td>\n",
       "      <td>3</td>\n",
       "      <td>76.131466</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>0.365884</td>\n",
       "      <td>5.192349</td>\n",
       "      <td>0.365884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>7973.6</td>\n",
       "      <td>911</td>\n",
       "      <td>0.059695</td>\n",
       "      <td>1.625806</td>\n",
       "      <td>1240</td>\n",
       "      <td>0.059695</td>\n",
       "      <td>40.577097</td>\n",
       "      <td>27.975000</td>\n",
       "      <td>17</td>\n",
       "      <td>73.467742</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.388589</td>\n",
       "      <td>5.304839</td>\n",
       "      <td>0.388589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>8435.4</td>\n",
       "      <td>1492</td>\n",
       "      <td>0.094791</td>\n",
       "      <td>1.726000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.094791</td>\n",
       "      <td>39.445333</td>\n",
       "      <td>24.013333</td>\n",
       "      <td>4</td>\n",
       "      <td>74.600000</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>0.446959</td>\n",
       "      <td>5.333500</td>\n",
       "      <td>0.446959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>8897.2</td>\n",
       "      <td>1136</td>\n",
       "      <td>0.048559</td>\n",
       "      <td>1.488848</td>\n",
       "      <td>1614</td>\n",
       "      <td>0.048559</td>\n",
       "      <td>35.556253</td>\n",
       "      <td>18.506494</td>\n",
       "      <td>10</td>\n",
       "      <td>70.384139</td>\n",
       "      <td>20.961039</td>\n",
       "      <td>0.431377</td>\n",
       "      <td>4.597893</td>\n",
       "      <td>0.431377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>9359.0</td>\n",
       "      <td>1364</td>\n",
       "      <td>0.067771</td>\n",
       "      <td>1.682914</td>\n",
       "      <td>1908</td>\n",
       "      <td>0.067771</td>\n",
       "      <td>36.757293</td>\n",
       "      <td>20.404762</td>\n",
       "      <td>2</td>\n",
       "      <td>71.488470</td>\n",
       "      <td>22.714286</td>\n",
       "      <td>0.514266</td>\n",
       "      <td>5.068134</td>\n",
       "      <td>0.514266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>9820.8</td>\n",
       "      <td>1302</td>\n",
       "      <td>0.054699</td>\n",
       "      <td>1.632808</td>\n",
       "      <td>1841</td>\n",
       "      <td>0.054699</td>\n",
       "      <td>35.454565</td>\n",
       "      <td>17.913978</td>\n",
       "      <td>24</td>\n",
       "      <td>70.722433</td>\n",
       "      <td>19.795699</td>\n",
       "      <td>0.452802</td>\n",
       "      <td>5.076046</td>\n",
       "      <td>0.452802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      URL_ID  COMPLEX WORD COUNT  POLARITY SCORE  SYLLABLES PER WORD  \\\n",
       "0    10282.6                1405       -0.002973            1.615464   \n",
       "1    10744.4                1063        0.056084            1.560027   \n",
       "2    11206.2                   0        0.000000            0.000000   \n",
       "3    12129.8                 602        0.115752            1.486989   \n",
       "4      123.0                1413        0.043159            1.588901   \n",
       "..       ...                 ...             ...                 ...   \n",
       "107   7973.6                 911        0.059695            1.625806   \n",
       "108   8435.4                1492        0.094791            1.726000   \n",
       "109   8897.2                1136        0.048559            1.488848   \n",
       "110   9359.0                1364        0.067771            1.682914   \n",
       "111   9820.8                1302        0.054699            1.632808   \n",
       "\n",
       "     WORD COUNT  POSITIVE SCORE  FOG INDEX  AVG SENTENCE LENGTH  \\\n",
       "0          1927       -0.002973  39.776626            26.530303   \n",
       "1          1466        0.056084  37.442802            21.096774   \n",
       "2             0        0.000000   0.000000             0.000000   \n",
       "3           807        0.115752  37.806477            19.918919   \n",
       "4          1856        0.043159  38.837586            20.962500   \n",
       "..          ...             ...        ...                  ...   \n",
       "107        1240        0.059695  40.577097            27.975000   \n",
       "108        2000        0.094791  39.445333            24.013333   \n",
       "109        1614        0.048559  35.556253            18.506494   \n",
       "110        1908        0.067771  36.757293            20.404762   \n",
       "111        1841        0.054699  35.454565            17.913978   \n",
       "\n",
       "     PERSONAL PRONOUNS  PERCENTAGE OF COMPLEX WORDS  \\\n",
       "0                   19                    72.911261   \n",
       "1                   30                    72.510232   \n",
       "2                    0                     0.000000   \n",
       "3                    1                    74.597274   \n",
       "4                    3                    76.131466   \n",
       "..                 ...                          ...   \n",
       "107                 17                    73.467742   \n",
       "108                  4                    74.600000   \n",
       "109                 10                    70.384139   \n",
       "110                  2                    71.488470   \n",
       "111                 24                    70.722433   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  NEGATIVE SCORE  AVG WORD LENGTH  \\\n",
       "0                           29.196970        0.517458         5.028542   \n",
       "1                           23.645161        0.423013         4.886767   \n",
       "2                            0.000000        0.000000         0.000000   \n",
       "3                           21.810811        0.556115         4.727385   \n",
       "4                           23.200000        0.365884         5.192349   \n",
       "..                                ...             ...              ...   \n",
       "107                         31.000000        0.388589         5.304839   \n",
       "108                         26.666667        0.446959         5.333500   \n",
       "109                         20.961039        0.431377         4.597893   \n",
       "110                         22.714286        0.514266         5.068134   \n",
       "111                         19.795699        0.452802         5.076046   \n",
       "\n",
       "     SUBJECTIVITY SCORE  \n",
       "0              0.517458  \n",
       "1              0.423013  \n",
       "2              0.000000  \n",
       "3              0.556115  \n",
       "4              0.365884  \n",
       "..                  ...  \n",
       "107            0.388589  \n",
       "108            0.446959  \n",
       "109            0.431377  \n",
       "110            0.514266  \n",
       "111            0.452802  \n",
       "\n",
       "[112 rows x 14 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = df.rename(columns={'File_Name' : 'URL_ID'})\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc550090",
   "metadata": {},
   "source": [
    "## Loading Output Data Structure Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea7fdb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLES PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>0.136034</td>\n",
       "      <td>0.437280</td>\n",
       "      <td>0.136034</td>\n",
       "      <td>0.437280</td>\n",
       "      <td>20.962500</td>\n",
       "      <td>76.131466</td>\n",
       "      <td>38.837586</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>1413</td>\n",
       "      <td>1856</td>\n",
       "      <td>1.588901</td>\n",
       "      <td>3</td>\n",
       "      <td>5.192349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>0.111801</td>\n",
       "      <td>0.615704</td>\n",
       "      <td>0.111801</td>\n",
       "      <td>0.615704</td>\n",
       "      <td>25.120000</td>\n",
       "      <td>73.823529</td>\n",
       "      <td>39.577412</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>502</td>\n",
       "      <td>680</td>\n",
       "      <td>1.682353</td>\n",
       "      <td>3</td>\n",
       "      <td>5.101471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>0.087857</td>\n",
       "      <td>0.460367</td>\n",
       "      <td>0.087857</td>\n",
       "      <td>0.460367</td>\n",
       "      <td>17.140625</td>\n",
       "      <td>74.363188</td>\n",
       "      <td>36.601525</td>\n",
       "      <td>19.015625</td>\n",
       "      <td>905</td>\n",
       "      <td>1217</td>\n",
       "      <td>1.614626</td>\n",
       "      <td>5</td>\n",
       "      <td>4.990961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>0.135696</td>\n",
       "      <td>0.386919</td>\n",
       "      <td>0.135696</td>\n",
       "      <td>0.386919</td>\n",
       "      <td>21.413793</td>\n",
       "      <td>76.410256</td>\n",
       "      <td>39.129620</td>\n",
       "      <td>23.534483</td>\n",
       "      <td>1043</td>\n",
       "      <td>1365</td>\n",
       "      <td>1.644689</td>\n",
       "      <td>12</td>\n",
       "      <td>5.306960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>0.135696</td>\n",
       "      <td>0.386919</td>\n",
       "      <td>0.135696</td>\n",
       "      <td>0.386919</td>\n",
       "      <td>21.413793</td>\n",
       "      <td>76.410256</td>\n",
       "      <td>39.129620</td>\n",
       "      <td>23.534483</td>\n",
       "      <td>1043</td>\n",
       "      <td>1365</td>\n",
       "      <td>1.644689</td>\n",
       "      <td>12</td>\n",
       "      <td>5.306960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>50921.0</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "      <td>0.079139</td>\n",
       "      <td>0.437437</td>\n",
       "      <td>0.079139</td>\n",
       "      <td>0.437437</td>\n",
       "      <td>23.586207</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>39.434483</td>\n",
       "      <td>26.344828</td>\n",
       "      <td>573</td>\n",
       "      <td>764</td>\n",
       "      <td>1.613874</td>\n",
       "      <td>0</td>\n",
       "      <td>5.141361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>51382.8</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "      <td>0.015383</td>\n",
       "      <td>0.400619</td>\n",
       "      <td>0.015383</td>\n",
       "      <td>0.400619</td>\n",
       "      <td>33.428571</td>\n",
       "      <td>74.221737</td>\n",
       "      <td>43.060123</td>\n",
       "      <td>37.367347</td>\n",
       "      <td>1359</td>\n",
       "      <td>1831</td>\n",
       "      <td>1.667395</td>\n",
       "      <td>2</td>\n",
       "      <td>5.260513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>51844.6</td>\n",
       "      <td>https://insights.blackcoffer.com/what-are-the-...</td>\n",
       "      <td>0.132965</td>\n",
       "      <td>0.455201</td>\n",
       "      <td>0.132965</td>\n",
       "      <td>0.455201</td>\n",
       "      <td>25.647059</td>\n",
       "      <td>74.559585</td>\n",
       "      <td>40.082658</td>\n",
       "      <td>28.382353</td>\n",
       "      <td>1439</td>\n",
       "      <td>1930</td>\n",
       "      <td>1.622798</td>\n",
       "      <td>0</td>\n",
       "      <td>5.055959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>52306.4</td>\n",
       "      <td>https://insights.blackcoffer.com/marketing-dri...</td>\n",
       "      <td>0.078913</td>\n",
       "      <td>0.429075</td>\n",
       "      <td>0.078913</td>\n",
       "      <td>0.429075</td>\n",
       "      <td>23.517241</td>\n",
       "      <td>71.915167</td>\n",
       "      <td>38.172963</td>\n",
       "      <td>26.827586</td>\n",
       "      <td>1119</td>\n",
       "      <td>1556</td>\n",
       "      <td>1.580334</td>\n",
       "      <td>5</td>\n",
       "      <td>5.044987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>52768.2</td>\n",
       "      <td>https://insights.blackcoffer.com/continued-dem...</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.446890</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.446890</td>\n",
       "      <td>24.487805</td>\n",
       "      <td>76.139410</td>\n",
       "      <td>40.250886</td>\n",
       "      <td>27.292683</td>\n",
       "      <td>852</td>\n",
       "      <td>1119</td>\n",
       "      <td>1.771224</td>\n",
       "      <td>1</td>\n",
       "      <td>5.433423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      URL_ID                                                URL  \\\n",
       "0      123.0  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "1      321.0  https://insights.blackcoffer.com/rise-of-e-hea...   \n",
       "2     2345.0  https://insights.blackcoffer.com/rise-of-e-hea...   \n",
       "3     4321.0  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "4      432.0  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "..       ...                                                ...   \n",
       "107  50921.0  https://insights.blackcoffer.com/coronavirus-i...   \n",
       "108  51382.8  https://insights.blackcoffer.com/coronavirus-i...   \n",
       "109  51844.6  https://insights.blackcoffer.com/what-are-the-...   \n",
       "110  52306.4  https://insights.blackcoffer.com/marketing-dri...   \n",
       "111  52768.2  https://insights.blackcoffer.com/continued-dem...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0          0.136034        0.437280        0.136034            0.437280   \n",
       "1          0.111801        0.615704        0.111801            0.615704   \n",
       "2          0.087857        0.460367        0.087857            0.460367   \n",
       "3          0.135696        0.386919        0.135696            0.386919   \n",
       "4          0.135696        0.386919        0.135696            0.386919   \n",
       "..              ...             ...             ...                 ...   \n",
       "107        0.079139        0.437437        0.079139            0.437437   \n",
       "108        0.015383        0.400619        0.015383            0.400619   \n",
       "109        0.132965        0.455201        0.132965            0.455201   \n",
       "110        0.078913        0.429075        0.078913            0.429075   \n",
       "111        0.056100        0.446890        0.056100            0.446890   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0              20.962500                    76.131466  38.837586   \n",
       "1              25.120000                    73.823529  39.577412   \n",
       "2              17.140625                    74.363188  36.601525   \n",
       "3              21.413793                    76.410256  39.129620   \n",
       "4              21.413793                    76.410256  39.129620   \n",
       "..                   ...                          ...        ...   \n",
       "107            23.586207                    75.000000  39.434483   \n",
       "108            33.428571                    74.221737  43.060123   \n",
       "109            25.647059                    74.559585  40.082658   \n",
       "110            23.517241                    71.915167  38.172963   \n",
       "111            24.487805                    76.139410  40.250886   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                           23.200000                1413        1856   \n",
       "1                           27.200000                 502         680   \n",
       "2                           19.015625                 905        1217   \n",
       "3                           23.534483                1043        1365   \n",
       "4                           23.534483                1043        1365   \n",
       "..                                ...                 ...         ...   \n",
       "107                         26.344828                 573         764   \n",
       "108                         37.367347                1359        1831   \n",
       "109                         28.382353                1439        1930   \n",
       "110                         26.827586                1119        1556   \n",
       "111                         27.292683                 852        1119   \n",
       "\n",
       "     SYLLABLES PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0              1.588901                  3         5.192349  \n",
       "1              1.682353                  3         5.101471  \n",
       "2              1.614626                  5         4.990961  \n",
       "3              1.644689                 12         5.306960  \n",
       "4              1.644689                 12         5.306960  \n",
       "..                  ...                ...              ...  \n",
       "107            1.613874                  0         5.141361  \n",
       "108            1.667395                  2         5.260513  \n",
       "109            1.622798                  0         5.055959  \n",
       "110            1.580334                  5         5.044987  \n",
       "111            1.771224                  1         5.433423  \n",
       "\n",
       "[112 rows x 15 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.read_excel('Output Data Structure.xlsx')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a4c7ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>50921.0</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>51382.8</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>51844.6</td>\n",
       "      <td>https://insights.blackcoffer.com/what-are-the-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>52306.4</td>\n",
       "      <td>https://insights.blackcoffer.com/marketing-dri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>52768.2</td>\n",
       "      <td>https://insights.blackcoffer.com/continued-dem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      URL_ID                                                URL\n",
       "0      123.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "1      321.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "2     2345.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "3     4321.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "4      432.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "..       ...                                                ...\n",
       "107  50921.0  https://insights.blackcoffer.com/coronavirus-i...\n",
       "108  51382.8  https://insights.blackcoffer.com/coronavirus-i...\n",
       "109  51844.6  https://insights.blackcoffer.com/what-are-the-...\n",
       "110  52306.4  https://insights.blackcoffer.com/marketing-dri...\n",
       "111  52768.2  https://insights.blackcoffer.com/continued-dem...\n",
       "\n",
       "[112 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_Frame = output[['URL_ID','URL']]\n",
    "Data_Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa2efd",
   "metadata": {},
   "source": [
    "## Merging DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c2c1285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SYLLABLES PER WORD</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>1413</td>\n",
       "      <td>0.043159</td>\n",
       "      <td>1.588901</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.043159</td>\n",
       "      <td>38.837586</td>\n",
       "      <td>20.962500</td>\n",
       "      <td>3</td>\n",
       "      <td>76.131466</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>0.365884</td>\n",
       "      <td>5.192349</td>\n",
       "      <td>0.365884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>502</td>\n",
       "      <td>0.059128</td>\n",
       "      <td>1.682353</td>\n",
       "      <td>680</td>\n",
       "      <td>0.059128</td>\n",
       "      <td>39.577412</td>\n",
       "      <td>25.120000</td>\n",
       "      <td>3</td>\n",
       "      <td>73.823529</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>0.636878</td>\n",
       "      <td>5.101471</td>\n",
       "      <td>0.636878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>905</td>\n",
       "      <td>0.062241</td>\n",
       "      <td>1.614626</td>\n",
       "      <td>1217</td>\n",
       "      <td>0.062241</td>\n",
       "      <td>36.601525</td>\n",
       "      <td>17.140625</td>\n",
       "      <td>5</td>\n",
       "      <td>74.363188</td>\n",
       "      <td>19.015625</td>\n",
       "      <td>0.447594</td>\n",
       "      <td>4.990961</td>\n",
       "      <td>0.447594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>1043</td>\n",
       "      <td>0.097318</td>\n",
       "      <td>1.644689</td>\n",
       "      <td>1365</td>\n",
       "      <td>0.097318</td>\n",
       "      <td>39.129620</td>\n",
       "      <td>21.413793</td>\n",
       "      <td>12</td>\n",
       "      <td>76.410256</td>\n",
       "      <td>23.534483</td>\n",
       "      <td>0.356921</td>\n",
       "      <td>5.306960</td>\n",
       "      <td>0.356921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>1043</td>\n",
       "      <td>0.097318</td>\n",
       "      <td>1.644689</td>\n",
       "      <td>1365</td>\n",
       "      <td>0.097318</td>\n",
       "      <td>39.129620</td>\n",
       "      <td>21.413793</td>\n",
       "      <td>12</td>\n",
       "      <td>76.410256</td>\n",
       "      <td>23.534483</td>\n",
       "      <td>0.356921</td>\n",
       "      <td>5.306960</td>\n",
       "      <td>0.356921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>50921.0</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "      <td>573</td>\n",
       "      <td>-0.028413</td>\n",
       "      <td>1.613874</td>\n",
       "      <td>764</td>\n",
       "      <td>-0.028413</td>\n",
       "      <td>39.434483</td>\n",
       "      <td>23.586207</td>\n",
       "      <td>0</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>26.344828</td>\n",
       "      <td>0.430536</td>\n",
       "      <td>5.141361</td>\n",
       "      <td>0.430536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>51382.8</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "      <td>1359</td>\n",
       "      <td>0.017776</td>\n",
       "      <td>1.667395</td>\n",
       "      <td>1831</td>\n",
       "      <td>0.017776</td>\n",
       "      <td>43.060123</td>\n",
       "      <td>33.428571</td>\n",
       "      <td>2</td>\n",
       "      <td>74.221737</td>\n",
       "      <td>37.367347</td>\n",
       "      <td>0.346267</td>\n",
       "      <td>5.260513</td>\n",
       "      <td>0.346267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>51844.6</td>\n",
       "      <td>https://insights.blackcoffer.com/what-are-the-...</td>\n",
       "      <td>1439</td>\n",
       "      <td>0.092848</td>\n",
       "      <td>1.622798</td>\n",
       "      <td>1930</td>\n",
       "      <td>0.092848</td>\n",
       "      <td>40.082658</td>\n",
       "      <td>25.647059</td>\n",
       "      <td>0</td>\n",
       "      <td>74.559585</td>\n",
       "      <td>28.382353</td>\n",
       "      <td>0.459253</td>\n",
       "      <td>5.055959</td>\n",
       "      <td>0.459253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>52306.4</td>\n",
       "      <td>https://insights.blackcoffer.com/marketing-dri...</td>\n",
       "      <td>1119</td>\n",
       "      <td>0.044177</td>\n",
       "      <td>1.580334</td>\n",
       "      <td>1556</td>\n",
       "      <td>0.044177</td>\n",
       "      <td>38.172963</td>\n",
       "      <td>23.517241</td>\n",
       "      <td>5</td>\n",
       "      <td>71.915167</td>\n",
       "      <td>26.827586</td>\n",
       "      <td>0.453750</td>\n",
       "      <td>5.044987</td>\n",
       "      <td>0.453750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>52768.2</td>\n",
       "      <td>https://insights.blackcoffer.com/continued-dem...</td>\n",
       "      <td>852</td>\n",
       "      <td>0.031442</td>\n",
       "      <td>1.771224</td>\n",
       "      <td>1119</td>\n",
       "      <td>0.031442</td>\n",
       "      <td>40.250886</td>\n",
       "      <td>24.487805</td>\n",
       "      <td>1</td>\n",
       "      <td>76.139410</td>\n",
       "      <td>27.292683</td>\n",
       "      <td>0.449327</td>\n",
       "      <td>5.433423</td>\n",
       "      <td>0.449327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      URL_ID                                                URL  \\\n",
       "0      123.0  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "1      321.0  https://insights.blackcoffer.com/rise-of-e-hea...   \n",
       "2     2345.0  https://insights.blackcoffer.com/rise-of-e-hea...   \n",
       "3     4321.0  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "4      432.0  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "..       ...                                                ...   \n",
       "107  50921.0  https://insights.blackcoffer.com/coronavirus-i...   \n",
       "108  51382.8  https://insights.blackcoffer.com/coronavirus-i...   \n",
       "109  51844.6  https://insights.blackcoffer.com/what-are-the-...   \n",
       "110  52306.4  https://insights.blackcoffer.com/marketing-dri...   \n",
       "111  52768.2  https://insights.blackcoffer.com/continued-dem...   \n",
       "\n",
       "     COMPLEX WORD COUNT  POLARITY SCORE  SYLLABLES PER WORD  WORD COUNT  \\\n",
       "0                  1413        0.043159            1.588901        1856   \n",
       "1                   502        0.059128            1.682353         680   \n",
       "2                   905        0.062241            1.614626        1217   \n",
       "3                  1043        0.097318            1.644689        1365   \n",
       "4                  1043        0.097318            1.644689        1365   \n",
       "..                  ...             ...                 ...         ...   \n",
       "107                 573       -0.028413            1.613874         764   \n",
       "108                1359        0.017776            1.667395        1831   \n",
       "109                1439        0.092848            1.622798        1930   \n",
       "110                1119        0.044177            1.580334        1556   \n",
       "111                 852        0.031442            1.771224        1119   \n",
       "\n",
       "     POSITIVE SCORE  FOG INDEX  AVG SENTENCE LENGTH  PERSONAL PRONOUNS  \\\n",
       "0          0.043159  38.837586            20.962500                  3   \n",
       "1          0.059128  39.577412            25.120000                  3   \n",
       "2          0.062241  36.601525            17.140625                  5   \n",
       "3          0.097318  39.129620            21.413793                 12   \n",
       "4          0.097318  39.129620            21.413793                 12   \n",
       "..              ...        ...                  ...                ...   \n",
       "107       -0.028413  39.434483            23.586207                  0   \n",
       "108        0.017776  43.060123            33.428571                  2   \n",
       "109        0.092848  40.082658            25.647059                  0   \n",
       "110        0.044177  38.172963            23.517241                  5   \n",
       "111        0.031442  40.250886            24.487805                  1   \n",
       "\n",
       "     PERCENTAGE OF COMPLEX WORDS  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                      76.131466                         23.200000   \n",
       "1                      73.823529                         27.200000   \n",
       "2                      74.363188                         19.015625   \n",
       "3                      76.410256                         23.534483   \n",
       "4                      76.410256                         23.534483   \n",
       "..                           ...                               ...   \n",
       "107                    75.000000                         26.344828   \n",
       "108                    74.221737                         37.367347   \n",
       "109                    74.559585                         28.382353   \n",
       "110                    71.915167                         26.827586   \n",
       "111                    76.139410                         27.292683   \n",
       "\n",
       "     NEGATIVE SCORE  AVG WORD LENGTH  SUBJECTIVITY SCORE  \n",
       "0          0.365884         5.192349            0.365884  \n",
       "1          0.636878         5.101471            0.636878  \n",
       "2          0.447594         4.990961            0.447594  \n",
       "3          0.356921         5.306960            0.356921  \n",
       "4          0.356921         5.306960            0.356921  \n",
       "..              ...              ...                 ...  \n",
       "107        0.430536         5.141361            0.430536  \n",
       "108        0.346267         5.260513            0.346267  \n",
       "109        0.459253         5.055959            0.459253  \n",
       "110        0.453750         5.044987            0.453750  \n",
       "111        0.449327         5.433423            0.449327  \n",
       "\n",
       "[112 rows x 15 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(Data_Frame, cleaned_df, on='URL_ID')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f1bc1",
   "metadata": {},
   "source": [
    "## Reordering Columns same as given in Output Data Structure Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "381b8b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLES PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>0.043159</td>\n",
       "      <td>0.365884</td>\n",
       "      <td>0.043159</td>\n",
       "      <td>0.365884</td>\n",
       "      <td>20.962500</td>\n",
       "      <td>76.131466</td>\n",
       "      <td>38.837586</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>1413</td>\n",
       "      <td>1856</td>\n",
       "      <td>1.588901</td>\n",
       "      <td>3</td>\n",
       "      <td>5.192349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>0.059128</td>\n",
       "      <td>0.636878</td>\n",
       "      <td>0.059128</td>\n",
       "      <td>0.636878</td>\n",
       "      <td>25.120000</td>\n",
       "      <td>73.823529</td>\n",
       "      <td>39.577412</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>502</td>\n",
       "      <td>680</td>\n",
       "      <td>1.682353</td>\n",
       "      <td>3</td>\n",
       "      <td>5.101471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>0.062241</td>\n",
       "      <td>0.447594</td>\n",
       "      <td>0.062241</td>\n",
       "      <td>0.447594</td>\n",
       "      <td>17.140625</td>\n",
       "      <td>74.363188</td>\n",
       "      <td>36.601525</td>\n",
       "      <td>19.015625</td>\n",
       "      <td>905</td>\n",
       "      <td>1217</td>\n",
       "      <td>1.614626</td>\n",
       "      <td>5</td>\n",
       "      <td>4.990961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>0.097318</td>\n",
       "      <td>0.356921</td>\n",
       "      <td>0.097318</td>\n",
       "      <td>0.356921</td>\n",
       "      <td>21.413793</td>\n",
       "      <td>76.410256</td>\n",
       "      <td>39.129620</td>\n",
       "      <td>23.534483</td>\n",
       "      <td>1043</td>\n",
       "      <td>1365</td>\n",
       "      <td>1.644689</td>\n",
       "      <td>12</td>\n",
       "      <td>5.306960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>0.097318</td>\n",
       "      <td>0.356921</td>\n",
       "      <td>0.097318</td>\n",
       "      <td>0.356921</td>\n",
       "      <td>21.413793</td>\n",
       "      <td>76.410256</td>\n",
       "      <td>39.129620</td>\n",
       "      <td>23.534483</td>\n",
       "      <td>1043</td>\n",
       "      <td>1365</td>\n",
       "      <td>1.644689</td>\n",
       "      <td>12</td>\n",
       "      <td>5.306960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...        0.043159   \n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...        0.059128   \n",
       "2  2345.0  https://insights.blackcoffer.com/rise-of-e-hea...        0.062241   \n",
       "3  4321.0  https://insights.blackcoffer.com/rise-of-telem...        0.097318   \n",
       "4   432.0  https://insights.blackcoffer.com/rise-of-telem...        0.097318   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0        0.365884        0.043159            0.365884            20.962500   \n",
       "1        0.636878        0.059128            0.636878            25.120000   \n",
       "2        0.447594        0.062241            0.447594            17.140625   \n",
       "3        0.356921        0.097318            0.356921            21.413793   \n",
       "4        0.356921        0.097318            0.356921            21.413793   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                    76.131466  38.837586                         23.200000   \n",
       "1                    73.823529  39.577412                         27.200000   \n",
       "2                    74.363188  36.601525                         19.015625   \n",
       "3                    76.410256  39.129620                         23.534483   \n",
       "4                    76.410256  39.129620                         23.534483   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLES PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                1413        1856            1.588901                  3   \n",
       "1                 502         680            1.682353                  3   \n",
       "2                 905        1217            1.614626                  5   \n",
       "3                1043        1365            1.644689                 12   \n",
       "4                1043        1365            1.644689                 12   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0         5.192349  \n",
       "1         5.101471  \n",
       "2         4.990961  \n",
       "3         5.306960  \n",
       "4         5.306960  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = merged_df\n",
    "\n",
    "Reordering_DataFrame = ['URL_ID', 'URL', 'POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLES PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']\n",
    "\n",
    "df = df[Reordering_DataFrame]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6efc8b5",
   "metadata": {},
   "source": [
    "## Saving Final Excel File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f5b2894",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Output Data Structure.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc3e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
